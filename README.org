* Overview
Overscore is an automatic score reader, that recognizes musical sheets
and plays them using [[http://overtone.github.com/][Overtone]]. Overscore consists of three main stages:
  - an /Optical Music Recognition/ (OMR) system to recognize
    partitions and compile them into MusicXML files
  - a MusicXML parser that compiles MusicXML files into Clojure files
  - multiple helper macros and functions built over Overtone to have a
    system more adapted to encode classical music
* How it works
** OMR
The initial plan for the OMR part is documented in
=doc/design/design.org=. In this section we will briefly describe how
it is currently implemented and discuss some results.
*** Part 1: Image Preprocessing
**** Binarization
Since the input image might be in grayscale or color, a binarization
step is sometimes necessary. However, most musical sheets found on the
internet are already binarized and this step is thus performed only if
necessary.

It is done in two steps:
  1. If the image is in color, convert it into grayscale using the
     [[http://en.wikipedia.org/wiki/Grayscale#Converting_color_to_grayscale][luma component]], implemented in [[overscore/tree/master/src/overscore/preprocessing/gray.clj][preprocessing/gray.clj]].
  2. Convert the grayscale image into binary using
     [[http://en.wikipedia.org/wiki/Otsu%27s_method][Otsu's method]], implemented in [[overscore/tree/master/src/overscore/preprocessing/otsu.clj][preprocessing/otsu.clj]].
**** Reference lengths
Once the image is binarized, the reference lengths (staff line and
staff space height) are found by analyzing the most common vertical
runs (black for staff line height, white for staff space height) as
described in [[RebeloFujinaga2012][Rebelo, Fujinaga et. al., 2012]]
**** Improvements
Multiple steps could be added to the preprocessing part. The most
useful would be a *skew detection and correction* step, since it is
common to have skewed images among scanned documents. Another possibly
useful step would be a *noise reduction* step, depending on the
quality of the scanned documents.
*** Part 2: Staff line processing
**** System isolation
The first step done by this part is to isolate the systems to process
them independently. This is done by a technique similar to the one
described in [[Fujinaga1988][Fujinaga, 1988]].

First, a y-projection of the pixel is done (see an example result in
[[overscore/tree/master/doc/yproj.png][doc/yproj.png]]), and the systems are located by looking for five
distinct peaks of black pixels. Then, the boundaries of the system are
found by looking around the system for the lines where the number of
black pixel is minimal.

Once each system is found, they are each saved in an image.

# image from doc/yproj.png generated by
# (def img (ImageIO/read (File. "/home/quentin/p/overscore/data/furelise.png")))
# (def p (projection img :y))
# (def chart
#   (set-background-alpha
#     (bar-chart (range (length p))
#        p :vertical false ) 0))
# (.setVisible (.getRangeAxis (.getCategoryPlot chart)) false)
# (.setVisible (.getDomainAxis (.getCategoryPlot chart)) false)
# (save chart "foo.png" :width 2745 :height 3611)
# then assembled with the png of the sheet

This step is implemented in [[overscore/tree/master/src/overscore/staffline/identification.clj][staffline/identification.clj]].
**** Staff line identification and removal
The staff line removal is done by using [[http://gamera.informatik.hsnr.de/index.html][Gamera]]'s [[http://music-staves.sf.net/][music-staves]]
plugin. A python script is simply called with the input image, and
outputs the same image without the staffline (in an image), as well as
the staff line positions (in a text file).

If no staff line were found in an image, it can be discarded since it
is most likely not a relevant image (eg. some text, like the title of
the partition).

This step is implemented in [[overscore/tree/master/src/overscore/staffline/removal.clj][staffline/removal.clj]] and calls the script
[[overscore/tree/master/src/overscore/staffline/removal.py][staffline/removal.py]].
**** Improvements
If the staff line removal does not work as expected because the image
is skewed, a skew correction algorithm should be implemented in the
[[Part 1: Image Preprocessing][preprocessing]].
*** Part 3: Symbol Recognition
**** Symbol Segmentation
The segmentation is done in a similar way as done in [[OpenOMR][OpenOMR]]. All the
sub parts of the segmentation process are assembled in
[[overscore/tree/master/src/recognition/segmentation/segmentation.clj][recognition/segmentation/segmentation.clj]], and takes as input a path
to an image (output by the preprocessing step), a path to a text files
containing the reference lengths (also part of the output of the
preprocessing step). It outputs all the segments represented as a
vector of 4-element vectors in a text file.
***** Level-0 Segmentation
The first segmentation is done by finding consecutive columns which
contains black pixels. The results are refined by:
  1. Grouping close segments, which can happen for example in the case
     of a dotted note
  2. Not taking small segments, which are probably due to noise in the
     scanned image.

The level-0 segmentation is implemented in [[overscore/tree/master/src/overscore/recognition/segmentation/level0.clj][recognition/segmentation/level0.clj]]. An
example of level-0 segmentation result can be found in
[[overscore/tree/master/doc/level0-segments.png][doc/level0-segments.png]].
***** Note Head Detection
For each level-0 segment, we need to know if it contains a note head
or not.

To detect if a segment contains note heads, the following algorithm is
used (taken from [[OpenOMR][OpenOMR]]):
  - For each column, find (if present) the biggest black run that is:
    - Smaller than 3/2 of the staffspace height
    - Bigger than 2 times the staffline height
    Remember the columns where such runs are present.
  - Find segments of columns having those black runs, such that the
    lengths of the segment is at least half of the staffspace
    height. Those segments correspond to the note heads.

Segments having note heads in it are further decomposed into multiple
level-1 segments. The others can directly be used as level-1 segments
without further decomposition.

The note head detection is implemented in [[overscore/tree/master/src/overscore/recognition/segmentation/notehead.clj][recognition/segmentation/notehead.clj]].
***** Level-1 Segmentation
Level-1 segmentation use the data computed by the note head detection:
for each note head found, it creates a level-1 segment. The space
between the note heads is also saved in a level-1 segment.

Level-1 segmentation is implemented in [[overscore/tree/master/src/overscore/recognition/segmentation/level1.clj][recognition/segmentation/level1.clj]] and an
example output on level-0 segments that contains notes can be found in
[[overscore/tree/master/doc/level1-segments.png][doc/level1-segments.png]].
***** Level-2 Segmentation
The level-2 segmentation separates the symbol contained in each
level-1 segment vertically. The resulting segments should then
correspond to the musical features (eg. a note head, a sharp, ...) and
can then be classified.

Level-2 segmentation is implemented in [[overscore/tree/master/src/recognition/segmentation/level2.clj][recognition/segmentation/level2.clj]].
**** Symbol recognition
Multiple symbol recognition methods are implemented. The one used by
default uses the [[https://en.wikipedia.org/wiki/K-nearest_neighbor_algorithm][k nearest neighbors algorithm]] provided by [[http://opencv.org/][OpenCV]],
using [[http://audiveris.kenai.com/][Audiveris]]' training set.

Since Audiveris store its training set as xml files describing
vertical runs for each image, we need to convert it to "normal" (2-bit
PNG) images (for easier manipulation). This is done in
[[overscore/tree/master/src/overscore/tools/audiveris.clj][tools/audiveris.clj]].

The training set is then loaded in
[[overscore/tree/master/src/recognition/classification/training.clj][recognition/classification/training.clj]], each image being resized to a
20x20 image and represented by a vector of 400 integer (1 meaning the
pixel is on (ie. black), 0 meaning it is off).

OpenCV's k nearest neighbor method is called directly from a C++
program, [[overscore/tree/master/src/recognition/classification/opencv_knn.cpp][recognition/classification/opencv_knn.cpp]], and the resulting
program is called from clojure in
[[overscore/tree/master/src/recognition/classification/opencv_knn.clj][recognition/classification/opencv_knn.clj]]. Once OpenCV is installed,
the C++ program can be compiled with:

#+BEGIN_SRC shell
$ g++ opencv_knn.cpp -o opencv_knn `pkg-config opencv --libs --cflags`
#+END_SRC

Another simple classifier using kNN (implemented by hand) is
implemented in [[overscore/tree/master/src/recognition/classification/knn.clj][recognition/classification/knn.clj]], and can use the
[[http://en.wikipedia.org/wiki/Hausdorff_distance][Hausdorff distance]] or the [[https://en.wikipedia.org/wiki/Euclidean_distance][Euclidian distance]]to compute the distance
between two images. The Hausdorff distance is implemented in
[[overscore/tree/master/src/recognition/classification/hausdorff.clj][recognition/classification/hausdorff.clj]], and the Euclidian distance
in [[overscore/tree/master/src/recognition/classification/euclidian.clj][recognition/classification/euclidian.clj]]. However, this
implementation of the kNN algorithm is *really* slow, and that is the
reason why OpenCV's kNN is used by default.

A neural network classifier using [[http://www.heatonresearch.com/][Encog]] is also implemented, in
[[overscore/tree/master/src/recognition/classification/nn.clj][recognition/classification/nn.clj]].

All the parts of the classification step are assembled in
[[overscore/tree/master/src/recognition/classification/classification.clj][recognition/classification/classification.clj]], and takes as input the
image (output by the preprocessing step) and a file describing the
segments (output by the segmentation step), and outputs a file
describing the class of each segment (as a vector of 5-element
vectors, where the 4 first elements are the segment description and
the last element is the class (as a symbol) of the vector)
**** Improvements
The segmentation might be improved by fine tuning the parameters. The
level-0 and level-1 segmentation works quite accurately, but the
level-2 segmentation performs really poorly at the moment.

The symbol recognition process is currently not accurate enough. It
might be because a big part (around 25%) of the training set consists
of black noteheads. This part could be reduced, and the rest of the
training set could be improved.

The kNN algorithm implemented by hand also suffers from huge performance
issues.

*** TODO Part 4: Musical Semantics
The musical semantics are defined by a set of rule, as the following
LL(1) grammar:

#+BEGIN_SRC text
<P> → <clef> <P'>

<P'> → <time> <notes>
       <notes>

<notes> → <note> <notes>
          ε

<note> → <pre> <note_body> <post>
         <rest>

<pre> → sharp
        flat
        natural

<post> → <flag>
         dot_set
     

<note_body> → <beam> <notehead>
              <notehead>

<time> → common_time
         cut_time
         time_four
         time_four_four
         time_six_eight
         time_three
         time_three_four
         time_two
         time_two_four

<clef> → g_clef
         g_clef_8vb
         f_clef
         c_clef

<rest> → eighth_rest
         one_16th_rest
         quarter_rest

<notehead> → notehead_black
             notehead_black_2
             notehead_black_3
             notehead_void
             notehead_void_2
             whole_note
             whole_note_2

<beam> → beam
         beam_hook

<flag> → flag_1
         flag_1_up
         flag_2
         flag_2_up
#+END_SRC
* Musical Notation
* Bibliography
The papers cited in this documentation are given in this section. For
more papers about the topic of OMR, see =doc/design/design.pdf=.

# <<RebeloFujinaga2012>>
  - A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. S. Marcal,
    C. Guedes, and J. S. Cardoso, /Optical Music Recognition -
    state-of-the-art and open issues/, 2012, [[http://www.inescporto.pt/~jsc/publications/journals/2012ARebeloIJMIR.pdf][link]].
# <<Fujinaga1988>>
  - I. Fujinaga, /Optical Music Recognition using Projections/, 1988,
    [[http://digitool.library.mcgill.ca/thesisfile61870.pdf][link]].
# <<OpenOMR>>
  - A. Desaedeleer, /Reading Sheet Music/, 2006, [[http://sourceforge.net/projects/openomr/][link to OpenOMR]] (pdf
    is included in the sources).
